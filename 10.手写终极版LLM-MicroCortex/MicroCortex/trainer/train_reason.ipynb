{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "trainer_reason.ipynb\n",
    "---------------------------------\n",
    "MicroCortex 语言模型思维链训练脚本（带详细中文注释）。\n",
    "本脚本演示了如何使用 PyTorch + Transformers 来思维链微调一个自定义的\n",
    "MicroCortexCausalLanguageModel，并支持分布式数据并行（DDP）训练、\n",
    "梯度累积、自动混合精度 (AMP)、学习率余弦退火以及按间隔保存检查点。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "一、导入相关包"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "__package__ = \"trainer\"\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "from torch import optim, nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from dataset.lm_dataset import SFTDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二、相关工具函数，和train_pretrain相同"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#日志打印函数，和train_pretrain相同，只有主进程打印\n",
    "def Logger(content):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)\n",
    "\n",
    "#余弦退火学习率，和train_pretrain相同\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "三、单个epoch的训练逻辑"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "模型与tokenizer的初始化，这里我们加载之前强化学习模型的参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_model(lm_config):\n",
    "    # 初始化tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../model')\n",
    "    #初始化模型\n",
    "    model = MicroCortexForCausalLM(lm_config)\n",
    "\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    # 模型名\n",
    "    ckp = f'{args.save_dir}/rlhf_{lm_config.hidden_size}{moe_path}.pth'\n",
    "    # 加载模型参数\n",
    "    state_dict = torch.load(ckp, map_location=args.device)\n",
    "    # 加载模型参数到模型\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    Logger(f'LLM可训练总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    model = model.to(args.device)\n",
    "    return model, tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "分布式初始化，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "单个epoch的训练，这里需要注意，因为训练数据中有思考标签，需要将思考标签的loss放大10倍"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(epoch, wandb):\n",
    "    # 思考标签token_id\n",
    "    start_of_think_ids = tokenizer('<think>').input_ids\n",
    "    end_of_think_ids = tokenizer('</think>').input_ids\n",
    "    start_of_answer_ids = tokenizer('<answer>').input_ids\n",
    "    end_of_answer_ids = tokenizer('</answer>').input_ids\n",
    "    # 使用 token‑level 交叉熵，和train_pretrain相同\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        # 将 batch 数据搬到指定设备\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        # 计算并设置学习率（step 级别）\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)\n",
    "        '''\n",
    "        optimizer.param_groups 是一个包含了所有模型参数组及其优化超参数（如学习率、动量等）的列表。\n",
    "        [\n",
    "          {\n",
    "            'params': [...],         # 这是模型的一部分参数（可以是一个列表，也可以是单个参数）\n",
    "            'lr': 0.001,             # 学习率\n",
    "            'weight_decay': 0.0005,  # 权重衰减\n",
    "            'momentum': 0.9,         # 如果是 SGD 可能会有\n",
    "            ...                      # 还有其他优化器相关的超参数\n",
    "          },\n",
    "          {\n",
    "            'params': [...],         # 这是模型的一部分参数（可以是一个列表，也可以是单个参数）\n",
    "            'lr': 0.001,             # 学习率\n",
    "            'weight_decay': 0.0005,  # 权重衰减\n",
    "            'momentum': 0.9,         # 如果是 SGD 可能会有\n",
    "            ...                      # 还有其他优化器相关的超参数\n",
    "           },\n",
    "           ...\n",
    "        ]\n",
    "        '''\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # 自动混合精度上下文(AMP Autocast)（在 CPU 时为 nullcontext 空操作）\n",
    "        # 自动混合精度训练会把反向传播前的loss乘以放大因子s，来避免(FP16/BP16)下的下溢\n",
    "        with ctx:\n",
    "            res = model(X)# 前向传播\n",
    "            # 交叉熵按 token 计算，随后与 mask 相乘，只统计非 padding 区域\n",
    "            # res.logits [batch, token, vocab_size] - >[batch*token, vocab_size]\n",
    "            # Y [batch, token] -> [batch*token]\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())# 再reshape回[batch, token]\n",
    "\n",
    "            # sp_ids bool类型的[batch*token]，对应Y.view(-1)，对应位置为True，表示Y中对应位置是思维标签\n",
    "            sp_ids = torch.isin(Y.view(-1),#[batch*token]\n",
    "                                torch.tensor(start_of_think_ids + end_of_think_ids\n",
    "                                             + start_of_answer_ids + end_of_answer_ids\n",
    "                                             ).to(args.device))#思维token_id列表\n",
    "            #！这里和train_pretrain不同：在 sp_ids 对应的位置增加额外的惩罚，loss_mask原本是0/1，\n",
    "            loss_mask = loss_mask.view(-1)#[batch, token] -> [batch*token]\n",
    "            # 有效 token 的个数\n",
    "            loss_mask_sum = loss_mask.sum()\n",
    "            #！这里和train_pretrain不同：思维标签位置权重放大10倍\n",
    "            loss_mask[sp_ids] = 10\n",
    "            # mask + 交叉熵求平均\n",
    "            loss_mask = loss_mask.view(Y.size())\n",
    "            loss = (loss * loss_mask).sum() / loss_mask_sum\n",
    "            # 加上模型可能返回的额外正则项（如 MoE loss）\n",
    "            loss += res.aux_loss\n",
    "            # 梯度累积：先除以累计步数\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        # 和train_pretrain相同\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(\n",
    "                    epoch + 1,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "\n",
    "            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n",
    "                wandb.log({\"loss\": loss * args.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if lm_config.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/reason_{lm_config.hidden_size}{moe_path}.pth'\n",
    "\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "\n",
    "            state_dict = {k: v.half() for k, v in state_dict.items()}  # 半精度保存\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "四、主函数入口\n",
    "启动示例（双卡）：\n",
    "torchrun –nporc_per_node 2 1-pretrain.py，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"MicroCortex Distill Reasoning\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"../out\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=6)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-7)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\")\n",
    "    parser.add_argument(\"--use_wandb\", action=\"store_true\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"MicroCortex-Full-SFT\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=1)\n",
    "    parser.add_argument(\"--ddp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0)\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=100)\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=100)\n",
    "    parser.add_argument('--local_rank', type=int, default=-1)\n",
    "    parser.add_argument('--hidden_size', default=512, type=int)\n",
    "    parser.add_argument('--num_hidden_layers', default=8, type=int)\n",
    "    parser.add_argument('--max_seq_len', default=1024, type=int)\n",
    "    parser.add_argument('--use_moe', default=True, type=bool)\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"../dataset/r1_mix_1024.jsonl\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ####################\n",
    "    #模型配置\n",
    "    ####################\n",
    "    lm_config = MicroCortexConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers,use_moe=args.use_moe)\n",
    "    # 创建输出目录\n",
    "    args.save_dir = os.path.join(args.out_dir)\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    tokens_per_iter = args.batch_size * args.max_seq_len\n",
    "    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "\n",
    "    # wandb run 名称 = 超参组合\n",
    "    args.wandb_run_name = f\"MiniMind-Full-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n",
    "\n",
    "    # AMP 上下文：CPU 下为 no‑op\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "\n",
    "    # 判断是否为 ddp 进程（RANK 环境变量由 torchrun 注入）\n",
    "    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n",
    "\n",
    "    ####################\n",
    "    #随机种子\n",
    "    ####################\n",
    "    base_seed = 1337\n",
    "    torch.manual_seed(base_seed)\n",
    "    torch.cuda.manual_seed(base_seed)\n",
    "\n",
    "    # 若为 DDP，需要调用初始化并根据 rank 调整种子/设备\n",
    "    if ddp:\n",
    "        init_distributed_mode()\n",
    "        args.device = torch.device(DEVICE)\n",
    "        rank = dist.get_rank()\n",
    "        torch.manual_seed(base_seed + rank)\n",
    "        # 同时设置 CUDA 的随机种子\n",
    "        torch.cuda.manual_seed(base_seed + rank)\n",
    "\n",
    "    ####################\n",
    "    #wandb初始化\n",
    "    ####################\n",
    "    if args.use_wandb and (not ddp or ddp_local_rank == 0):\n",
    "        import wandb\n",
    "\n",
    "        # 初始化wandb的项目和训练名\n",
    "        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n",
    "    else:\n",
    "        wandb = None\n",
    "\n",
    "    ####################\n",
    "    #模型、数据集初始化\n",
    "    ####################\n",
    "    model, tokenizer = init_model(lm_config)\n",
    "\n",
    "    train_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)\n",
    "    # 若为分布式，使用 DistributedSampler 保证各进程拿到不同切片\n",
    "    train_sampler = DistributedSampler(train_ds) if ddp else None\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    # AMP 梯度缩放器\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    # AdamW 优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 若启用 DDP，包裹模型；排除不需要同步的 pos_cis（静态预计算矩阵），但是其实原作者这里有错误，它的旋转矩阵变量注册名并不是pos_cis\n",
    "    if ddp:\n",
    "        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n",
    "        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])\n",
    "    # 每 epoch 的 step 数\n",
    "    iter_per_epoch = len(train_loader)\n",
    "\n",
    "    ####################\n",
    "    #开始训练\n",
    "    ####################\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch, wandb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "一、首先导入相关包"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "from .model_microcortex import *\n",
    "from typing import Optional, Tuple, List\n",
    "from torch import nn\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from typing import List\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二、定义模型配置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VLMConfig(MicroCortexConfig):\n",
    "    model_type = \"microcortex-v\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_special_token: str = '@' * 196,\n",
    "            image_ids: List = [34] * 196,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.image_special_token = image_special_token\n",
    "        self.image_ids = image_ids\n",
    "        super().__init__(**kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "三、定义模型结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VisionProj(nn.Module):\n",
    "    def __init__(self, ve_hidden_size=768, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.ve_hidden_size = ve_hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vision_proj = nn.Sequential(\n",
    "            nn.Linear(self.ve_hidden_size, self.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_encoders):\n",
    "        vision_proj = self.vision_proj(image_encoders)\n",
    "        return vision_proj"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 继承自语言模型\n",
    "class MicroCortexVLM(MicroCortexForCausalLM):\n",
    "    config_class = VLMConfig\n",
    "\n",
    "    def __init__(self, params: VLMConfig = None, vision_model_path=\"./model/vision_model/clip-vit-base-patch16\"):\n",
    "        super().__init__(params)\n",
    "        if not params: params = VLMConfig()\n",
    "        self.params = params\n",
    "        self.vision_encoder, self.processor = self.__class__.get_vision_model(vision_model_path)\n",
    "        self.vision_proj = VisionProj(hidden_size=params.hidden_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_vision_model(model_path: str):\n",
    "        from transformers import logging as hf_logging\n",
    "        hf_logging.set_verbosity_error()\n",
    "        if not os.path.exists(model_path):\n",
    "            return None, None\n",
    "        model = CLIPModel.from_pretrained(model_path)\n",
    "        processor = CLIPProcessor.from_pretrained(model_path)\n",
    "        # 冻结 vision_encoder 的所有参数\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        return model.eval(), processor\n",
    "\n",
    "    @staticmethod\n",
    "    def image2tensor(image, processor):\n",
    "        if image.mode in ['RGBA', 'LA']: image = image.convert('RGB')\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")['pixel_values']\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_image_embeddings(image_tensors, vision_model):\n",
    "        with torch.no_grad():\n",
    "            outputs = vision_model.vision_model(pixel_values=image_tensors)\n",
    "        img_embedding = outputs.last_hidden_state[:, 1:, :].squeeze()\n",
    "        return img_embedding\n",
    "\n",
    "    def count_vision_proj(self, tokens, h, vision_tensors=None, seqlen=512):\n",
    "        def find_indices(tokens, image_ids):\n",
    "            image_ids_tensor = torch.tensor(image_ids).to(tokens.device)\n",
    "            len_image_ids = len(image_ids)\n",
    "            if len_image_ids > tokens.size(1):\n",
    "                return None\n",
    "            tokens_view = tokens.unfold(1, len_image_ids, 1)\n",
    "            matches = (tokens_view == image_ids_tensor).all(dim=2)\n",
    "            return {\n",
    "                batch_idx: [(idx.item(), idx.item() + len_image_ids - 1) for idx in\n",
    "                            matches[batch_idx].nonzero(as_tuple=True)[0]]\n",
    "                for batch_idx in range(tokens.size(0)) if matches[batch_idx].any()\n",
    "            } or None\n",
    "\n",
    "        image_indices = find_indices(tokens, self.params.image_ids)\n",
    "        if vision_tensors is not None and image_indices:\n",
    "            vision_proj = self.vision_proj(vision_tensors)\n",
    "            if len(vision_proj.shape) == 3:\n",
    "                vision_proj = vision_proj.unsqueeze(0)\n",
    "            new_h = []\n",
    "            for i in range(h.size(0)):\n",
    "                if i in image_indices:\n",
    "                    h_i = h[i]\n",
    "                    img_idx = 0\n",
    "                    for start_idx, end_idx in image_indices[i]:\n",
    "                        if img_idx < vision_proj.size(1):\n",
    "                            h_i = torch.cat((h_i[:start_idx], vision_proj[i][img_idx], h_i[end_idx + 1:]), dim=0)[\n",
    "                                  :seqlen]\n",
    "                            img_idx += 1\n",
    "                    new_h.append(h_i)\n",
    "                else:\n",
    "                    new_h.append(h[i])\n",
    "            return torch.stack(new_h, dim=0)\n",
    "        return h\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,\n",
    "                use_cache: bool = False,\n",
    "                logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "                pixel_values: Optional[torch.FloatTensor] = None,\n",
    "                **args):\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        past_key_values = past_key_values or [None] * len(self.model.layers)\n",
    "        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0\n",
    "\n",
    "        hidden_states = self.model.dropout(self.model.embed_tokens(input_ids))\n",
    "\n",
    "        if pixel_values is not None and start_pos == 0:\n",
    "            if len(pixel_values.shape) == 6:\n",
    "                pixel_values = pixel_values.squeeze(2)\n",
    "            bs, num, c, im_h, im_w = pixel_values.shape\n",
    "            stack_dim = 1 if bs > 1 else 0\n",
    "            vision_tensors = torch.stack([\n",
    "                MicroCortexVLM.get_image_embeddings(pixel_values[:, i, :, :, :], self.vision_encoder)\n",
    "                for i in range(num)\n",
    "            ], dim=stack_dim)\n",
    "            hidden_states = self.count_vision_proj(tokens=input_ids, h=hidden_states, vision_tensors=vision_tensors,\n",
    "                                                   seqlen=input_ids.shape[1])\n",
    "\n",
    "        position_embeddings = (\n",
    "            self.model.freqs_cos[start_pos:start_pos + seq_length],\n",
    "            self.model.freqs_sin[start_pos:start_pos + seq_length]\n",
    "        )\n",
    "\n",
    "        presents = []\n",
    "        for layer_idx, (layer, past_key_value) in enumerate(zip(self.model.layers, past_key_values)):\n",
    "            hidden_states, present = layer(\n",
    "                hidden_states,\n",
    "                position_embeddings,\n",
    "                past_key_value=past_key_value,\n",
    "                use_cache=use_cache,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            presents.append(present)\n",
    "\n",
    "        hidden_states = self.model.norm(hidden_states)\n",
    "\n",
    "        aux_loss = sum(\n",
    "            layer.mlp.aux_loss\n",
    "            for layer in self.model.layers\n",
    "            if isinstance(layer.mlp, MicroCortexMOE)\n",
    "        )\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "        self.OUT.__setitem__('last_hidden_state', hidden_states)\n",
    "        self.OUT.__setitem__('logits', logits)\n",
    "        self.OUT.__setitem__('aux_loss', aux_loss)\n",
    "        self.OUT.__setitem__('past_key_values', presents)\n",
    "        return self.OUT"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

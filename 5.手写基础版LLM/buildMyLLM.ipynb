{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "一、首先导入相关的包"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二、定义模型配置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size:int = 512            #处理文本的最大长度（max_seq_len）\n",
    "    batch_size:int = 12\n",
    "    n_layer:int = 6                 #6层block\n",
    "    n_head:int = 12                 #多头注意力头数\n",
    "    n_embed:int = 768               #词嵌入向量维度，这里为了tie_embedding_weight，所以embed维度和hidden_size相同\n",
    "    hidden_dim:int = n_embed\n",
    "    head_size:int =n_embed//n_head  #多头注意力头大小\n",
    "    dropout:float = 0.1\n",
    "    vocab_size:int = 50257          #tiktoken，使用的是gpt-2的官方的tokenzier，所以vocab_size是50257"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "三、现在来定义模型结构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1、首先定义单头注意力层 single head attention"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class singleHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        #定义qkv三个线性变换\n",
    "        self.key=nn.Linear(config.hidden_dim,config.head_size)\n",
    "        self.query=nn.Linear(config.hidden_dim,config.head_size)\n",
    "        self.value=nn.Linear(config.hidden_dim,config.head_size)\n",
    "        #head_size就是当前注意力头 层的输出维度\n",
    "        self.head_size=config.head_size\n",
    "\n",
    "        #下三角矩阵，通过register_buffer注册\n",
    "        #因为不用计算梯度，所以节约内存和显存，速度也更快\n",
    "        self.register_buffer(\n",
    "            'attention_mask',\n",
    "            #block_size是512，就是文本的最大长度\n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size,config.block_size)\n",
    "            )\n",
    "        )\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,hidden_size=x.size()\n",
    "        k=self.key(x)\n",
    "        v=self.value(x)\n",
    "        q=self.query(x)\n",
    "        weight=q@k.transpose(-2,-1)\n",
    "        # weight得分矩阵中，所有“掩码为0”的元素设置为'-inf'，从而实现对这些位置完全屏蔽\n",
    "        weight=weight.masked_fill(\n",
    "            #将attention_mask下三角矩阵裁剪到seq_len*seq_len，因为实际的seq_len可能比block_size小\n",
    "            self.attention_mask[:seq_len,:seq_len]==0,\n",
    "            float('-inf')\n",
    "        )\n",
    "        #在最后一个维度进行softmax\n",
    "        #注意要除以根号下d_k，head_size就是当前的hidden_size\n",
    "        weight=F.softmax(weight,dim=-1)/math.sqrt(self.head_size)\n",
    "        #dropout要放在weight后面，而不是放在output后面\n",
    "        weight = self.dropout(weight)\n",
    "        output=weight@v\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2、现在定义多头注意力，多头注意力就是进行多次自注意力后，拼接一下结果，然后再进行全连接一下\n",
    "但是其实多头注意力的写法有更加优雅的通过矩阵转置的实现方式，这里暂且不表"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class multiHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        #有多少个头就有多少个自注意力计算\n",
    "        self.heads=nn.ModuleList(\n",
    "            [\n",
    "                singleHeadAttention(config)\n",
    "                for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj=nn.Linear(config.hidden_dim,config.hidden_dim)\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #对每个头进行自注意力计算后拼接起来\n",
    "        output=torch.cat(\n",
    "            [h(x) for h in self.heads],dim=-1\n",
    "        )\n",
    "        output=self.proj(output)#全连接一下\n",
    "        output=self.dropout(output)#dropout一下即可\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3、再定义一下前馈层（feedFroward、MLP），其实就是一个全连接"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class feedForward(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            #hidden_dim-》4*hidden_dim-》GELU-》hidden_dim-》Dropout\n",
    "            nn.Linear(config.hidden_dim,4*config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*config.hidden_dim,config.hidden_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4、现在来定义一个完整的Block吧，一个完整的block就是先ln1，再注意力，再ln2，再ffn，记得要残差连接"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "   def __init__(self,config):\n",
    "       super().__init__()\n",
    "       head_size=config.n_embed//config.n_head\n",
    "       self.att=multiHeadAttention(config)\n",
    "       self.ffn=feedForward(config)\n",
    "       self.ln1=nn.LayerNorm(config.hidden_dim)\n",
    "       self.ln2=nn.LayerNorm(config.hidden_dim)\n",
    "\n",
    "   def forward(self,x):\n",
    "        x=x+self.att(self.ln1(x))\n",
    "        x=x+self.ffn(self.ln2(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5、现在我们开始构建完整的gpt model，完整的model需要词嵌入、位置嵌入"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        #词嵌入、位置嵌入、norm、mlp、blocks\n",
    "        #   现在的大模型把position embedding从0，1，xxxembedding升级到rope\n",
    "        #   norm 从 layer norm升级到rms norm\n",
    "        #   mlp升级到swiglu\n",
    "        #   mha升级到gqa\n",
    "        #   后面我们也会一步一步升级\n",
    "        self.token_embedding_table=nn.Embedding(config.vocab_size,config.n_embed)\n",
    "        self.position_embedding_table=nn.Embedding(config. ,config.n_embed)\n",
    "\n",
    "\n",
    "        self.blocks=nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "\n",
    "        self.ln_final=nn.LayerNorm(config.n_embed)\n",
    "\n",
    "        #lm_head层输出的是词表中每个词的分数，可以与embedding层贡献参数\n",
    "        self.lm_head=nn.Linear(config.n_embed,config.vocab_size,bias=False)\n",
    "\n",
    "        #Linear(4->8);weight shape实际上是8*4，在实际计算中是h@E的转置，\n",
    "        # 而embedding中不用转置，就直接是8*4，所以token_embedding_table的weight和lm_head的weight可以直接相等\n",
    "        #所以embedding weight和lm_head weight是共享的\n",
    "        #这里学习一下tie weight，这是为了减少参数，加快训练，现在很多SLM都是这样做的\n",
    "        self.token_embedding_table.weight=self.lm_head.weight\n",
    "\n",
    "        #初始化全部参数\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self,module):\n",
    "        #如果是线形层，就有bias\n",
    "        if isinstance(module,nn.Linear):\n",
    "            #初始化为正态分布\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        #如果是embedding层就没有bias\n",
    "        elif isinstance(module,nn.Embedding):\n",
    "            #初始化为正态分布\n",
    "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
    "\n",
    "    def forward(self,input_ids,labels=None):\n",
    "        #input_ids就是输入的token ids\n",
    "        #labels是目标token ids\n",
    "        #shape要一样\n",
    "        batch,seq_len=input_ids.size()#(batch,seq_len)\n",
    "        token_embed=self.token_embedding_table(input_ids)# shape is (batch_size , seq_len , n_embd)\n",
    "\n",
    "        #seq长度是这次输入的最大长度\n",
    "        pos_embed=self.position_embedding_table(torch.arange(seq_len,device=input_ids.device))#要确保位置编码和输入的input_ids在一个设备上\n",
    "\n",
    "        #经典题目，为什么embedding和positionembedding可以相加\n",
    "        x=token_embed+pos_embed     #shape is (batch_szie, seq_len, n_embd)\n",
    "        x=self.blocks(x)\n",
    "        x=self.ln_final(x)\n",
    "        logits=self.lm_head(x)      #shape is (batch_szie, seq_len, vocab_size)\n",
    "\n",
    "        if labels is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            batch,seq_len,vocab_size=logits.size()\n",
    "            logits=logits.view(batch*seq_len,vocab_size)       #shape : (batch_szie, seq_len, vocab_size)->(batch_szie*seq_len, vocab_size)\n",
    "            labels=labels.view(batch*seq_len)     #shape : (batch_szie*seq_len)\n",
    "            loss=F.cross_entropy(logits,labels)\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        #idx是(batch_size,seq_len)的数组\n",
    "        for _ in range(max_new_tokens):\n",
    "            #如果序列太长，就只取最后block_size个token\n",
    "            idx_cond=idx if idx.size(1)<=self.block_size else idx[:,-self.block_size:]\n",
    "            #获取预测\n",
    "            logits,_=self(idx_cond)\n",
    "            #shape (batch_size, seq_len, vocab_size)\n",
    "            #只关注最后一个时间步的预测\n",
    "            logits=logits[:,-1,:]   #(batch_size,vocab_size)\n",
    "            #使用softmax获取概率\n",
    "            probs=F.softmax(logits,dim=-1)\n",
    "            #采样下一个token\n",
    "            idx_next=torch.multinomial(probs,num_samples=1)\n",
    "            #附加到序列上\n",
    "            idx=torch.cat((idx,idx_next),dim=-1) #shape (batch_size, seq_len+1)\n",
    "        return idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "四、构建输入的DataSet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "了解模型的输入是什么样子的"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self,path,max_length=512):\n",
    "        #我的数据在/root/fs/mobvoi_seq_monkey_general_open_corpus.jsonl 中，\n",
    "        #读取前1000行\n",
    "        import tiktoken     #gpt官方的tokenizer，这里是在代码中写死了tokenizer，其实应该作为参数来指定更好\n",
    "        self.enc=tiktoken.get_encoding(\"gpt2\")\n",
    "        self.max_length=max_length\n",
    "\n",
    "        #请注意这里使用的方式是将所有训练数据编码再加上<|endoftext|>后拼接成一个超长序列，然后再以max_length来切分成一个一个样本，其实也可以直接以原始的每一行数据作为一个样本，长切短补。\n",
    "        #其中第一种方式更加适合预训练，第二种方式更加适合后期微调和强化学习\n",
    "\n",
    "        #用特殊符号分割不同的文本\n",
    "        #<|endoftext|>\n",
    "        self.eos_token=self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={ \"<|endoftext|>\"}\n",
    "        )\n",
    "\n",
    "        import json\n",
    "        self.encoded_data=[]\n",
    "\n",
    "        self.max_lines=1000\n",
    "        raw_data=[]\n",
    "        #打开数据集文件\n",
    "        with open(path,'r')as f:\n",
    "            #按行遍历数据集\n",
    "            for i,line in enumerate(f):\n",
    "                #这里暂时只取前一千行数据，因为内存不够\n",
    "                if i>=self.max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    #获取键“text”对应的值\n",
    "                    text=json.loads(line.strip())['text']\n",
    "                    #加入raw_data列表中\n",
    "                    raw_data.append(text)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        full_encoded=[]\n",
    "        #遍历raw_data每一行句子\n",
    "        for text in raw_data:\n",
    "            #将句子编码为token\n",
    "            encoded_text=self.enc.encode(text)\n",
    "            #在编码最后放入eos_token后拼接到full_encoded超长序列中\n",
    "            full_encoded.extend(encoded_text+[self.eos_token])\n",
    "        #现在这个full_encoded就是全部训练数据的token的列表了\n",
    "\n",
    "        #长到短（max_length：512）\n",
    "        #将full_encoded长文本分割成训练样本，以self.max_length为大小遍历full_encoded\n",
    "        for i in range(0,len(full_encoded),self.max_length):\n",
    "            #获取一个token作为目标\n",
    "            chunk=full_encoded[i:i+self.max_length+1]#多取一个，用来方便我们做target\n",
    "            #如果长度不够，用eos_token填充，当然也可以直接丢弃\n",
    "            if len(chunk)<self.max_length+1:\n",
    "                chunk=chunk+[self.eos_token]*(self.max_length+1-len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample=self.encoded_data[idx]\n",
    "        x=torch.tensor(sample[:-1],dtype=torch.long)\n",
    "        y=torch.tensor(sample[1:],dtype=torch.long)\n",
    "        return x,y\n",
    "\n",
    "    def encode(self,text):\n",
    "        #将文本编码转成token ids\n",
    "        return self.enc.encode(text)\n",
    "\n",
    "    def decode(self,ids):\n",
    "        #将token ids转成文本编码\n",
    "        return self.enc.encode(ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数据格式\n",
    "\"\"\"\n",
    "{\"text\":\"担任地点省市的区域运营中心的办理作业。承受总部相关KPI查核。\\n1、了解新闻职业或媒体相关运营运营岗位，其间，应聘区域运营中心主任有3年以上当地干流媒体作业经验者优先，应聘事务主管有2年以上当地干流媒体作业经验者优先。\\n2、交流才能强，抗压才能强，长于处理复杂情况，了解GR作业优先，能独立完结策划计划优先。具有独立开发客户才能。\\n北京、天津、河北、山西、黑龙江、吉林、辽宁、上海、江苏、浙江、安徽、江西、福建、山东、河南、湖北、湖南、广东、海南、重庆、四川、贵州、云南、陕西等。\"}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "五、运行相关函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model=GPT(GPTConfig())\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model=model.to(device)\n",
    "\n",
    "#打印模型一共有多少参数\n",
    "total_params=sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:{total_params/1e6}M\")\n",
    "\n",
    "#设置优化器\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=3e-4)\n",
    "#设置cosine学习率\n",
    "scheduler=torch.optim.lr_scheduler.ConsineAnnealingLR(optimizer,T_max=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "创建我们的训练和验证的dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train data\n",
    "train_dataset=myDataset('/root/fs/mobvoi_seq_monkey_general_open_corpus.jsonl')\n",
    "\n",
    "#分割数据集\n",
    "train_dataset,val_dataset=torch.utils.data.random_split(train_dataset,[0.9,0.1])\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=12,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=12,shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在开始训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#循环训练\n",
    "def train(model,optimizer,scheduler,train_loader,val_loader,device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for batch_idx,(x,y) in enumerate(train_loader):\n",
    "        #将数据转移到设备上\n",
    "        x,y=x.to(device),y.to(device)\n",
    "\n",
    "        #前向传播\n",
    "        logits,loss=model(x,targets=y)\n",
    "\n",
    "        #反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #调整学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        if batch_idx%100==0:\n",
    "            print(f'Epoch:{epoch},batch:{batch_idx},loss:{loss.item():.4f}')\n",
    "    return total_loss\n",
    "\n",
    "def eval(model,val_loader,device):\n",
    "    #验证\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_loader:\n",
    "            x,y=x.to(device),y.to(device)\n",
    "            logits,loss=model(x,targets=y)\n",
    "            val_loss+=loss.item()\n",
    "    return val_loss\n",
    "\n",
    "for epoch in range(2):\n",
    "    train_loss=train(model,optimizer,scheduler,train_loader,val_loader,device)\n",
    "    val_loss=eval(model,val_loader,device)\n",
    "    print(f'Epoch:{epoch}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "    # 保存模型\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "    }\n",
    "    # 保存每个epoch的模型\n",
    "    torch.save(checkpoint, f'checkpoints/model_epoch_{epoch}.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

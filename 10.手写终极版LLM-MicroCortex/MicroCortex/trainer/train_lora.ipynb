{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "trainer_full_sft.ipynb\n",
    "---------------------------------\n",
    "MicroCortex 语言模型全参数微调脚本（带详细中文注释）。\n",
    "本脚本演示了如何使用 PyTorch + Transformers 来SFT微调一个自定义的\n",
    "MicroCortex Causal Language Model，并支持分布式数据并行（DDP）训练、\n",
    "梯度累积、自动混合精度 (AMP)、学习率余弦退火以及按间隔保存检查点。\n",
    "这里除了创建模型的时候和train_pretrain不同，因为这里要加载之前的预训练模型权重，其他基本完全一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "一、导入相关包，"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "__package__ = \"trainer\"\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from dataset.lm_full_sft_dataset import SFTDataset\n",
    "from model.model_lora import load_lora, save_lora, apply_lora\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二、相关的工具函数，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "日志打印函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Logger(content):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "余弦退火学习率调整（带warmup/10起始）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "三、单个eopch的训练逻辑"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "模型与tokenizer的初始化，这里我们是加载之前full_sft模型的参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "    model = MiniMindForCausalLM(lm_config)\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'\n",
    "    state_dict = torch.load(ckp, map_location=args.device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    return model.to(args.device), tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "分布式初始化，torchrun环境变量自动注入，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "单个epoch的训练，和full_sft「几乎」一致"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(epoch, wandb):\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            res = model(X)\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            loss += res.aux_loss\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(lora_params, args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(\n",
    "                    epoch + 1,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "\n",
    "            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n",
    "                wandb.log({\"loss\": loss * args.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n",
    "            model.eval()\n",
    "            lora_save_path = f'{args.save_dir}/lora/{args.lora_name}_{lm_config.hidden_size}.pth'\n",
    "            os.makedirs(os.path.dirname(lora_save_path), exist_ok=True)\n",
    "            # 【区别1】只保存lora权重即可\n",
    "            save_lora(model, lora_save_path)\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "四、主函数入口\n",
    "启动示例（双卡）：\n",
    "torchrun –nporc_per_node 2 1-pretrain.py，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"MiniMind SFT with LoRA\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"../out\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\")\n",
    "    parser.add_argument(\"--use_wandb\", action=\"store_true\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"MiniMind-LoRA-SFT\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=1)\n",
    "    parser.add_argument(\"--ddp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0)\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=100)\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=100)\n",
    "    parser.add_argument('--local_rank', type=int, default=-1)\n",
    "    parser.add_argument('--hidden_size', default=512, type=int)\n",
    "    parser.add_argument('--num_hidden_layers', default=8, type=int)\n",
    "    parser.add_argument('--max_seq_len', default=512, type=int)\n",
    "    parser.add_argument('--use_moe', default=False, type=bool)\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"../dataset/lora_medical.jsonl\")\n",
    "    parser.add_argument(\"--lora_name\", type=str, default=\"lora_medical\", help=\"根据任务保存成lora_(英文/医学/心理...)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers,\n",
    "                               use_moe=args.use_moe)\n",
    "    args.save_dir = os.path.join(args.out_dir)\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    tokens_per_iter = args.batch_size * args.max_seq_len\n",
    "    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n",
    "    base_seed = 1337\n",
    "    torch.manual_seed(base_seed)\n",
    "    torch.cuda.manual_seed(base_seed)\n",
    "\n",
    "    if ddp:\n",
    "        init_distributed_mode()\n",
    "        args.device = torch.device(DEVICE)\n",
    "        rank = dist.get_rank()\n",
    "        torch.manual_seed(base_seed + rank)\n",
    "        # 同时设置 CUDA 的随机种子\n",
    "        torch.cuda.manual_seed(base_seed + rank)\n",
    "\n",
    "    args.wandb_run_name = f\"MiniMind-Lora-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n",
    "    if args.use_wandb and (not ddp or ddp_local_rank == 0):\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n",
    "    else:\n",
    "        wandb = None\n",
    "\n",
    "    model, tokenizer = init_model(lm_config)\n",
    "    apply_lora(model)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())  # 总参数数量\n",
    "    lora_params_count = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name)  # LoRA 参数数量\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(f\"LLM 总参数量: {total_params}\")\n",
    "        print(f\"LoRA 参数量: {lora_params_count}\")\n",
    "        print(f\"LoRA 参数占比: {lora_params_count / total_params * 100:.2f}%\")\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' not in name:\n",
    "            param.requires_grad = False\n",
    "    lora_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            lora_params.append(param)\n",
    "\n",
    "    # 只对 LoRA 参数进行优化\n",
    "    optimizer = optim.AdamW(lora_params, lr=args.learning_rate)\n",
    "    train_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)\n",
    "    train_sampler = DistributedSampler(train_ds) if ddp else None\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    iter_per_epoch = len(train_loader)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch, wandb)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

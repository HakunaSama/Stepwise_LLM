{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "trainer_distillation.ipynb\n",
    "---------------------------------\n",
    "MicroCortex 语言模型模型蒸馏脚本（带详细中文注释）。\n",
    "本脚本演示了如何使用 PyTorch + Transformers 来蒸馏一个自定义的\n",
    "MicroCortexCausalLanguageModel，并支持分布式数据并行（DDP）训练、\n",
    "梯度累积、自动混合精度 (AMP)、学习率余弦退火以及按间隔保存检查点。\n",
    "\"\"\"\n",
    "模型加载：\n",
    "    学生模型512*8权重full_sft_512.pth\n",
    "    教师模型768*16权重full_sft_768.pth\n",
    "    学生参与优化，教师仅前向传递参数与蒸馏\n",
    "损失：\n",
    "    训练损失 = α × CE(学生 vs GT) + (1–α) × KL(学生 || 教师) @ temperature，蒸馏部分仅在 mask=1 的 token 上计算。\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "一、导入相关包，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "__package__ = \"trainer\"\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "from torch import optim\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from dataset.lm_dataset import SFTDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二、相关的工具函数，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Logger(content):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)\n",
    "\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "kl散度损失"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# kl散度损失\n",
    "def distillation_loss_fn(student_logits, teacher_logits, temperature=1.0, reduction='batchmean'):\n",
    "    '''\n",
    "    计算蒸馏损失（KL 散度）\n",
    "    Args:\n",
    "        student_logits: [N, V] 学生网络最后一层未归一化的logits\n",
    "        teacher_logits: [N, V] 教师网络最后一层为归一化的logits\n",
    "        temperature: float 温度系数，温度越低分布越平缓，温度越高分布越陡峭\n",
    "        reduction: str kl散度的聚合方式\n",
    "    Raise:\n",
    "        loss:\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        # 输出教师模型的概率分布，这里用到了温度\n",
    "        teacher_probs = F.softmax(teacher_logits / temperature, hidden_size=-1).detach()\n",
    "\n",
    "    student_log_probs = F.log_softmax(student_logits / temperature, hidden_size=-1)\n",
    "\n",
    "    kl = F.kl_div(\n",
    "        student_log_probs,\n",
    "        teacher_probs,\n",
    "        reduction=reduction\n",
    "    )\n",
    "    # 因为teacher_probs和student_log_probs都除以了T，所以要乘以T平方，使梯度大小与温度设置无关\n",
    "    # 当T为1的时候就相当于完全没有T\n",
    "    return (temperature ** 2) * kl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "三、单个epoch的训练逻辑"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "模型与tokenizer的初始化，这里我们要初始化两个模型，老师模型和学生模型，这里我们是加载之前SFT模型的参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#初始化学生和教师模型\n",
    "def init_student_model(lm_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "    model = MiniMindForCausalLM(lm_config)\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'\n",
    "    state_dict = torch.load(ckp, map_location=args.device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    Logger(f'学生模型(LLM)总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def init_teacher_model(lm_config):\n",
    "    model = MiniMindForCausalLM(lm_config)\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'\n",
    "    state_dict = torch.load(ckp, map_location=args.device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    Logger(f'教师模型(LLM)总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    model = model.to(args.device)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "分布式初始化，torchrun环境变量自动注入，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#和train_pretrain一样\n",
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "单个epoch的训练，"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(epoch, wandb, alpha=0.0, temperature=1.0):\n",
    "    start_time = time.time()\n",
    "\n",
    "    #教师模型固定参数，即用eval模式\n",
    "    if teacher_model is not None:\n",
    "        teacher_model.eval()\n",
    "        teacher_model.requires_grad_(False)#教师模型也不计算梯度\n",
    "\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        # 将 batch 数据搬到指定设备\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        # 计算并设置学习率（step 级别）\n",
    "        lr = get_lr(epoch * iter_per_epoch + step,\n",
    "                    args.epochs * iter_per_epoch,\n",
    "                    args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # 前向传播（学生模型），自动混合精度上下文（在 CPU 时为 nullcontext 空操作）\n",
    "        with ctx:\n",
    "            res = model(X)\n",
    "            student_logits = res.logits\n",
    "\n",
    "        # 教师模型前向传播（只在eval & no_grad）\n",
    "        if teacher_model is not None:\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(X).logits\n",
    "                vocab_size_student = student_logits.size(-1)  # N\n",
    "                teacher_logits = teacher_logits[..., :vocab_size_student]\n",
    "\n",
    "        # ========== 计算损失 ==========\n",
    "        #### 1) Ground-Truth CE Loss（可选）\n",
    "        loss_mask_flat = loss_mask.view(-1)\n",
    "        #计算学生模型输出和Y的交叉熵损失，token level级别的，这和train_pretrain是一样的\n",
    "        ce_loss = F.cross_entropy(\n",
    "            student_logits.view(-1, student_logits.size(-1)),\n",
    "            Y.view(-1),\n",
    "            ignore_index=0,\n",
    "            reduction='none'\n",
    "        )\n",
    "        # mask + 求平均\n",
    "        ce_loss = torch.sum(ce_loss * loss_mask_flat) / loss_mask_flat.sum()\n",
    "        # 加上模型可能返回的额外正则项（如 MoE loss）\n",
    "        if lm_config_student.use_moe:\n",
    "            ce_loss += res.aux_loss\n",
    "\n",
    "        #### 2) Distillation Loss（可选）\n",
    "        if teacher_model is not None:\n",
    "            # 只在有效token位置做蒸馏\n",
    "            distill_loss = distillation_loss_fn(\n",
    "                student_logits.view(-1, student_logits.size(-1))[loss_mask_flat == 1],\n",
    "                teacher_logits.view(-1, teacher_logits.size(-1))[loss_mask_flat == 1],\n",
    "                temperature=temperature\n",
    "            )\n",
    "        else:\n",
    "            distill_loss = torch.tensor(0.0, device=args.device)\n",
    "\n",
    "        #### 3) 总损失 = alpha * CE + (1-alpha) * Distill\n",
    "        # 梯度累积：先除以累计步数\n",
    "        loss = (alpha * ce_loss + (1 - alpha) * distill_loss) / args.accumulation_steps\n",
    "\n",
    "        #以下开始和train_pretrain就一样了\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.4f} lr:{:.12f} epoch_Time:{}min:'.format(\n",
    "                    epoch,\n",
    "                    args.epochs - 1,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item(),\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"ce_loss\": ce_loss.item(),\n",
    "                    \"distill_loss\": distill_loss.item() if teacher_model is not None else 0.0,\n",
    "                    \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                    \"last-time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60\n",
    "                })\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if lm_config_student.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/full_dist_{lm_config_student.hidden_size}{moe_path}.pth'\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "            state_dict = {k: v.half() for k, v in state_dict.items()}  # 半精度保存\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "一、最简单的版本"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self,hidden_dim:int = 512) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        self.query_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self,hidden_states):# hidden_states [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        q=self.query_proj(hidden_states)# q [batch_size, seq_len, hidden_dim]\n",
    "        k = self.key_proj(hidden_states)# k [batch_size, seq_len, hidden_dim]\n",
    "        v = self.value_proj(hidden_states)# v [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        attention_value=torch.matmul(# attention_value [batch_size, seq_len, seq_len]\n",
    "            q,k.transpose(-1,-2)# k 转置后 [batch_size, hidden_dim, seq_len]\n",
    "        )\n",
    "\n",
    "        attention_weight=torch.softmax(# attention_weight [batch_size, seq_len, seq_len]\n",
    "            attention_value/math.sqrt(self.hidden_dim),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        print(attention_weight)\n",
    "\n",
    "        # output [batch_size, seq_len, hidden_dim]\n",
    "        output=torch.matmul(attention_weight.v)\n",
    "\n",
    "        return output\n",
    "\n",
    "x=torch.rand(3,2,4)\n",
    "self_att_net=SelfAttentionV1(4)\n",
    "self_att_net(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二、效率优化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self,hidden_dim:int = 512) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        self.proj=nn.Linear(hidden_dim,hidden_dim*3)\n",
    "\n",
    "    def forward(self, hidden_states):  # hidden_states [batch_size, seq_len, hidden_dim]\n",
    "        qkv=self.proj(hidden_states) # qkv [batch_size, seq_len, hidden_dim*3]\n",
    "        q,k,v=torch.split(qkv,self.hidden_dim,dim=-1)\n",
    "\n",
    "        attention_weight = torch.softmax(# attention_weight [batch_size, seq_len, seq_len]\n",
    "            torch.matmul(  # attention_value [batch_size, seq_len, seq_len]\n",
    "                q, k.transpose(-1, -2)  # k 转置后 [batch_size, hidden_dim, seq_len]\n",
    "            )/math.sqrt(self.hidden_dim)\n",
    "        )\n",
    "\n",
    "        print(attention_weight)\n",
    "\n",
    "        # output [batch_size, seq_len, hidden_dim]\n",
    "        output=attention_weight@v\n",
    "\n",
    "        return output\n",
    "\n",
    "x=torch.rand(3,2,4)\n",
    "self_att_net=SelfAttentionV2(4)\n",
    "self_att_net(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "三、#加入dropout、attention_mask、output线性变换"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 512, dropout_rate=0.1, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim * 3)\n",
    "        self.attention_dropout=nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.output_proj=nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):  # hidden_states [batch_size, seq_len, hidden_dim]\n",
    "        qkv = self.proj(hidden_states)  # qkv [batch_size, seq_len, hidden_dim*3]\n",
    "        q, k, v = torch.split(qkv, self.hidden_dim, dim=-1)\n",
    "\n",
    "        attention_value=q@k.transpos(-1,-2)/math.sqrt(self.hidden_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_value=attention_value.marked_fill(\n",
    "                attention_mask==0,\n",
    "                float(\"-1e20\")\n",
    "            )\n",
    "\n",
    "        attention_weight=torch.softmax(attention_value,dim=-1)\n",
    "\n",
    "        attention_weight=self.attention_dropout(attention_weight)\n",
    "        print(attention_weight)\n",
    "\n",
    "        # output [batch_size, seq_len, hidden_dim]\n",
    "        output = attention_weight @ v\n",
    "\n",
    "        output=self.output_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "x=torch.rand(3,4,2)\n",
    "# mask [batch_size, seq_len, seq_len] [3,4,4]\n",
    "mask=torch.tensor(\n",
    "    [\n",
    "        [1,1,1,0],\n",
    "        [1,1,0,0],\n",
    "        [1,0,0,0]\n",
    "    ]\n",
    ")\n",
    "mask=mask.unsqueeze(dim=1).repeat(1,4,1)\n",
    "\n",
    "self_att_net=SelfAttentionV3(2,mask)\n",
    "self_att_net(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "四、面试写法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SelfAttentionInterview(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 512, dropout_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        self.query_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self,hidden_states,attention_mask=None):# hidden_states [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        q = self.query_proj(hidden_states)  # q [batch_size, seq_len, hidden_dim]\n",
    "        k = self.key_proj(hidden_states)  # k [batch_size, seq_len, hidden_dim]\n",
    "        v = self.value_proj(hidden_states)  # v [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        attention_value = q @ k.transpos(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_value=attention_value.marked_fill(\n",
    "                attention_mask==0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "\n",
    "        attention_weight = torch.softmax(attention_value, dim=-1)\n",
    "\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "\n",
    "        # output [batch_size, seq_len, hidden_dim]\n",
    "        output = attention_weight @ v\n",
    "\n",
    "        output = self.output_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "x=torch.rand(3,4,2)\n",
    "# mask [batch_size, seq_len, seq_len] [3,4,4]\n",
    "mask=torch.tensor(\n",
    "    [\n",
    "        [1,1,1,0],\n",
    "        [1,1,0,0],\n",
    "        [1,0,0,0]\n",
    "    ]\n",
    ")\n",
    "mask=mask.unsqueeze(dim=1).repeat(1,4,1)\n",
    "\n",
    "self_att_net=SelfAttentionV3(2,mask)\n",
    "self_att_net(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

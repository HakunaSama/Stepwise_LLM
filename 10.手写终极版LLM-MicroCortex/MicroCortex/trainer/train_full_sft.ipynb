{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "trainer_full_sft.ipynb\n",
    "---------------------------------\n",
    "MicroCortex 语言模型全参数微调脚本（带详细中文注释）。\n",
    "本脚本演示了如何使用 PyTorch + Transformers 来SFT微调一个自定义的\n",
    "MicroCortex Causal Language Model，并支持分布式数据并行（DDP）训练、\n",
    "梯度累积、自动混合精度 (AMP)、学习率余弦退火以及按间隔保存检查点。\n",
    "这里除了创建模型的时候和train_pretrain不同，因为这里要加载之前的预训练模型权重，其他基本完全一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "一、导入相关包，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "__package__ = \"trainer\"\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "from torch import optim, nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model.model_microcortex import MicroCortexConfig, MicroCortexForCausalLM\n",
    "# 导入SFTDataset\n",
    "from dataset.llm_full_sft_dataset import SFTDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二、相关的工具函数，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "日志打印函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Logger(content):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "余弦退火学习率调整（带warmup/10起始）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "三、单个eopch的训练逻辑"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "模型与tokenizer的初始化，这里我们是加载之前预训练模型的参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_model(lm_config):\n",
    "    # 初始化tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../model')\n",
    "    #初始化模型\n",
    "    model = MicroCortexForCausalLM(lm_config)\n",
    "\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "    # 模型名\n",
    "    ckp = f'{args.save_dir}/pretrain_{lm_config.hidden_size}{moe_path}.pth'\n",
    "    # 加载模型参数\n",
    "    state_dict = torch.load(ckp, map_location=args.device)\n",
    "    # 加载模型参数到模型\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    Logger(f'LLM可训练总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')\n",
    "    model = model.to(args.device)\n",
    "    return model, tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "分布式初始化，torchrun环境变量自动注入，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "单个epoch的训练，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(epoch, wandb):\n",
    "    # 依旧使用 token‑level 交叉熵；不在这里求平均以便后续乘以 loss_mask\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    start_time = time.time()\n",
    "    # 从数据集中取出一个batch的X，Y，loss_mask，lossmask和padding是对应的\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        # 将batch数据全部搬到指定设备上\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "\n",
    "        # 计算并设置学习率（step级别）\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            #前向传播\n",
    "            res = model(X)\n",
    "            # 计算tokenlevel的交叉熵损失\n",
    "            loss = loss_fct(res.logits.view(-1, res.logits.size(-1)),Y.view(-1)).view(Y.size())\n",
    "            # 只统计训练数据中模型回答区域的token_level级别交叉熵，然后求平均\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            # 损失加上MoE的正则loss\n",
    "            loss += res.aux_loss\n",
    "            #当前minibatch的梯度，要除以batch/minibatch\n",
    "            loss = loss / args.accumulation_steps\n",
    "\n",
    "        # 自动混合精度训练会把反向传播前的loss乘以放大因子，来避免(FP16/BP16)下的下溢出\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 每 accumulation_steps 个 minibatch 更新一下权重\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            # 梯度除以放大因子，将梯度恢复到真实尺度\n",
    "            scaler.unscale_(optimizer)\n",
    "            # 裁剪过大梯度\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            #执行优化器更新参数并且更新scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            #清空梯度节约显存\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # 日志打印与wandb可视化\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(\n",
    "                    epoch + 1,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "            # 同步到wandb\n",
    "            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n",
    "                wandb.log({\"loss\": loss * args.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if lm_config.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'\n",
    "            # DDP 下需要取 module\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "\n",
    "            # 半精度存储，节约磁盘空间\n",
    "            state_dict = {k: v.half() for k, v in state_dict.items()}  # 半精度保存\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "四、主函数入口\n",
    "启动示例（双卡）：\n",
    "torchrun –nporc_per_node 2 1-pretrain.py，和train_pretrain一样"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"MicroCortex Full SFT\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"../out\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=6)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-7)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\")\n",
    "    parser.add_argument(\"--use_wandb\", action=\"store_true\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"MicroCortex-Full-SFT\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=1)\n",
    "    parser.add_argument(\"--ddp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0)\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=100)\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=100)\n",
    "    parser.add_argument('--local_rank', type=int, default=-1)\n",
    "    parser.add_argument('--hidden_size', default=512, type=int)\n",
    "    parser.add_argument('--num_hidden_layers', default=8, type=int)\n",
    "    parser.add_argument('--max_seq_len', default=1024, type=int)\n",
    "    parser.add_argument('--use_moe', default=True, type=bool)\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"../dataset/sft_mini_512.jsonl\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ####################\n",
    "    #模型配置\n",
    "    ####################\n",
    "    lm_config = MicroCortexConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers,use_moe=args.use_moe)\n",
    "    # 创建输出目录\n",
    "    args.save_dir = os.path.join(args.out_dir)\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    tokens_per_iter = args.batch_size * args.max_seq_len\n",
    "    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "\n",
    "    # wandb run 名称 = 超参组合\n",
    "    args.wandb_run_name = f\"MicroCortex-Full-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n",
    "\n",
    "    # AMP 上下文：CPU 下为 no‑op\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "\n",
    "    # 判断是否为 ddp 进程（RANK 环境变量由 torchrun 注入）\n",
    "    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n",
    "\n",
    "    ####################\n",
    "    #随机种子\n",
    "    ####################\n",
    "    base_seed = 1337\n",
    "    torch.manual_seed(base_seed)\n",
    "    torch.cuda.manual_seed(base_seed)\n",
    "\n",
    "    # 若为 DDP，需要调用初始化并根据 rank 调整种子/设备\n",
    "    if ddp:\n",
    "        init_distributed_mode()\n",
    "        args.device = torch.device(DEVICE)\n",
    "        rank = dist.get_rank()\n",
    "        torch.manual_seed(base_seed + rank)\n",
    "        # 同时设置 CUDA 的随机种子\n",
    "        torch.cuda.manual_seed(base_seed + rank)\n",
    "\n",
    "    ####################\n",
    "    #wandb初始化\n",
    "    ####################\n",
    "    if args.use_wandb and (not ddp or ddp_local_rank == 0):\n",
    "        import wandb\n",
    "\n",
    "        # 初始化wandb的项目和训练名\n",
    "        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n",
    "    else:\n",
    "        wandb = None\n",
    "\n",
    "    ####################\n",
    "    #模型、数据集初始化\n",
    "    ####################\n",
    "    model, tokenizer = init_model(lm_config)\n",
    "\n",
    "    train_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)\n",
    "    # 若为分布式，使用 DistributedSampler 保证各进程拿到不同切片\n",
    "    train_sampler = DistributedSampler(train_ds) if ddp else None\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    # AMP 梯度缩放器\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    # AdamW 优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 若启用 DDP，包裹模型；排除不需要同步的 pos_cis（静态预计算矩阵），但是其实原作者这里有错误，它的旋转矩阵变量注册名并不是pos_cis\n",
    "    if ddp:\n",
    "        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n",
    "        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])\n",
    "    # 每 epoch 的 step 数\n",
    "    iter_per_epoch = len(train_loader)\n",
    "\n",
    "    ####################\n",
    "    #开始训练\n",
    "    ####################\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch, wandb)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
